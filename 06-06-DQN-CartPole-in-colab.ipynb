{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "06-06-DQN-CartPole-in-colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantae74/Reinforcement-Learning/blob/main/06-06-DQN-CartPole-in-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgQSbgOoe-2"
      },
      "source": [
        "모두를 위한 머신러닝에서 가져왔습니다.\n",
        "# CartPole 강화학습 by DQN in Colab"
      ],
      "id": "WfgQSbgOoe-2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Dce3qdzAQN"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "id": "D8Dce3qdzAQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK3QTbWDofS7"
      },
      "source": [
        "from  collections import deque\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "id": "RK3QTbWDofS7",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFr28bjiyy2n"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "id": "DFr28bjiyy2n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgGZuFkzzW-k",
        "outputId": "2135a79b-11df-4141-f6a8-f22db834727c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.version.VERSION"
      ],
      "id": "rgGZuFkzzW-k",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dxHmEPog-b"
      },
      "source": [
        "MAX_EPISODE = 20000 # @param {type:\"integer\"}\n",
        "DISCOUNT_RATE = 0.99  # @param {type:\"number\"} \n",
        "REPLAY_MEMORY = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "BATCH_SIZE = 64  # @param {type:\"integer\"}\n",
        "TRAIN_START = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "# minimum epsilon for epsilon greedy\n",
        "MIN_E = 0.0 # @param {type:\"number\"} \n",
        "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
        "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n",
        "\n"
      ],
      "id": "30dxHmEPog-b",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqPCiJBh0VfI"
      },
      "source": [
        "# Load the CartPole environment from the OpenAI Gym suite\n",
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "\n",
        "INPUT_SIZE = env.observation_space.shape[0]\n",
        "OUTPUT_SIZE = env.action_space.n"
      ],
      "id": "zqPCiJBh0VfI",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48QHPanV7rcG",
        "outputId": "6e654611-9604-4929-89ea-ed08fdb193c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"input_size:\", INPUT_SIZE)\n",
        "print(\"output_size:\", OUTPUT_SIZE)\n",
        "print(\"input_size:\", env.observation_space.shape)"
      ],
      "id": "48QHPanV7rcG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_size: 4\n",
            "output_size: 2\n",
            "input_size: (4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMVyWAsn03cz"
      },
      "source": [
        "def OurModel(input_shape, action_space):\n",
        "    print(\"start OurModel\")\n",
        "\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # 'Dense' is the basic form of a neural network layer\n",
        "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
        "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
        "\n",
        "    # Hidden layer with 256 nodes\n",
        "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "    \n",
        "    # Hidden layer with 64 nodes\n",
        "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    # Output Layer with # of actions: 2 nodes (left, right)\n",
        "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    print(\"before compile\")\n",
        "\n",
        "#     model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "    model.summary()\n",
        "    print(\"End OurModel\")\n",
        "    return model"
      ],
      "id": "SMVyWAsn03cz",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWOQpQCX1l5X"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, input_size, output_size, name = \"main\"):\n",
        "        self.state_size = input_size\n",
        "        self.action_size = output_size\n",
        "        self.net_name = name\n",
        "        \n",
        "        self.model = OurModel(input_shape=(self.state_size,), action_space=self.action_size)\n",
        "    \n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)"
      ],
      "id": "UWOQpQCX1l5X",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnSJiD1e018T"
      },
      "source": [
        "def replay(mainDQN, train_batch):\n",
        "\n",
        "    states = np.vstack(x[0] for x in train_batch)\n",
        "    actions = np.array(x[1] for x in train_batch)\n",
        "    rewards = np.varray(x[2] for x in train_batch)\n",
        "    next_states = np.vstack(x[3] for x in train_batch)\n",
        "    dones = np.array(x[4] for x in train_batch)\n",
        "\n",
        "    target = mainDQN.predict(states)\n",
        "    target_next = mainDQN.predict(next_states)\n",
        "\n",
        "    for i in range(BATCH_SIZE):\n",
        "      if done[i]:\n",
        "        target[i][actions[i]] = reward[i]\n",
        "      else:\n",
        "        target[i][actions[i]] = rewards[i] + DISCOUNT_RATE * np.max(target_next[i])\n",
        "\n",
        "    mainDQN.fit(state, target,batch_size=BATCH_SIZE, verbose=0)"
      ],
      "id": "RnSJiD1e018T",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D-fh1c42QX8"
      },
      "source": [
        "def run(mainDQN):\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "\n",
        "  while True:\n",
        "    env.render()\n",
        "    action = np.argmax(mainDQN.predict(state))\n",
        "    state, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    if done:\n",
        "      print(\"Total score: {}\".format(total_reward))\n",
        "      break"
      ],
      "id": "6D-fh1c42QX8",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFRvi7U12UIr"
      },
      "source": [
        "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
        "\n",
        "    slope = (min_e - max_e) / (target_episode)\n",
        "    intercept = max_e\n",
        "\n",
        "    return max(min_e, slope * episode + intercept)"
      ],
      "id": "PFRvi7U12UIr",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4a-rZn32cxy"
      },
      "source": [
        "def main():\n",
        "  print(\"start main\")\n",
        "  replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "  last_100_game_reward = deque(maxlen=100)\n",
        "\n",
        "  print(\"before DQN\")\n",
        "  mainDQN = DQN(INPUT_SIZE, OUTPUT_SIZE)\n",
        "  print(\"after DQN\")\n",
        "\n",
        "  for episode in range(MAX_EPISODE):\n",
        "    e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
        "    done = False\n",
        "\n",
        "    state = env.reset()\n",
        "    PIL.Image.fromarray(env.render())\n",
        "\n",
        "    step_count = 0\n",
        "    while not done:\n",
        "      \n",
        "      if np.random.rand() < e:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "\n",
        "      if done:\n",
        "        reward = -1\n",
        "      \n",
        "      replay_buffer.append(state, action, reward, next_state, done)\n",
        "\n",
        "      state = next_state\n",
        "      step_count += 1\n",
        "\n",
        "      if len(replay_buffer) > TRAIN_START:\n",
        "        minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
        "        replay(mainDQN, minibatch)\n",
        "\n",
        "    print(\"[Episode {:>5}] steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
        "\n",
        "\n",
        "    last_100_game_reward.append(step_count)\n",
        "    if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
        "        avg_reward = np.mean(last_100_game_reward)\n",
        "        if avg_reward > 199.0:\n",
        "            print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
        "            break"
      ],
      "id": "q4a-rZn32cxy",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYeWOXAj2cQQ",
        "outputId": "d4f0ecaf-808d-47f8-b28d-f88b69417aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "id": "bYeWOXAj2cQQ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start main\n",
            "before DQN\n",
            "start OurModel\n",
            "before compile\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               2560      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 150,466\n",
            "Trainable params: 150,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "End OurModel\n",
            "after DQN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-20a4c88e53ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-4cf00d7ad22a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after DQN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannealing_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON_DECAYING_EPISODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MAX_EPISODES' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1podVFEb4lDf"
      },
      "source": [
        ""
      ],
      "id": "1podVFEb4lDf",
      "execution_count": null,
      "outputs": []
    }
  ]
}