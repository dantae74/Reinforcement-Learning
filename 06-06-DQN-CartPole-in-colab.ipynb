{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "06-06-DQN-CartPole-in-colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantae74/Reinforcement-Learning/blob/main/06-06-DQN-CartPole-in-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgQSbgOoe-2"
      },
      "source": [
        "모두를 위한 머신러닝에서 가져왔습니다.\n",
        "# CartPole 강화학습 by DQN in Colab"
      ],
      "id": "WfgQSbgOoe-2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Dce3qdzAQN"
      },
      "source": [
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y xvfb ffmpeg\n",
        "# !pip install 'imageio==2.4.0'\n",
        "# !pip install pyvirtualdisplay\n",
        "# !pip install tf-agents"
      ],
      "id": "D8Dce3qdzAQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK3QTbWDofS7"
      },
      "source": [
        "from  collections import deque\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "import pyvirtualdisplay\n",
        "from tf_agents.environments import suite_gym\n",
        "import PIL.Image"
      ],
      "id": "RK3QTbWDofS7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFr28bjiyy2n"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "id": "DFr28bjiyy2n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgGZuFkzzW-k"
      },
      "source": [
        "tf.version.VERSION"
      ],
      "id": "rgGZuFkzzW-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dxHmEPog-b"
      },
      "source": [
        "MAX_EPISODE = 20000 # @param {type:\"integer\"}\n",
        "DISCOUNT_RATE = 0.99  # @param {type:\"number\"} \n",
        "REPLAY_MEMORY = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "LEARNING_RATE =  1e-3  # @param {type:\"number\"}\n",
        "\n",
        "BATCH_SIZE = 64  # @param {type:\"integer\"}\n",
        "TRAIN_START = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "# minimum epsilon for epsilon greedy\n",
        "MIN_E = 0.0 # @param {type:\"number\"} \n",
        "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
        "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n",
        "\n"
      ],
      "id": "30dxHmEPog-b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqPCiJBh0VfI"
      },
      "source": [
        "# Load the CartPole environment from the OpenAI Gym suite\n",
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "\n",
        "INPUT_SIZE = env.observation_space.shape[0]\n",
        "OUTPUT_SIZE = env.action_space.n"
      ],
      "id": "zqPCiJBh0VfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48QHPanV7rcG"
      },
      "source": [
        "print(\"input_size:\", INPUT_SIZE)\n",
        "print(\"output_size:\", OUTPUT_SIZE)\n",
        "print(\"input_size:\", env.observation_space.shape)"
      ],
      "id": "48QHPanV7rcG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMVyWAsn03cz"
      },
      "source": [
        "def OurModel(input_shape, action_space):\n",
        "    print(\"start OurModel\")\n",
        "\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # 'Dense' is the basic form of a neural network layer\n",
        "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
        "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
        "\n",
        "    # Hidden layer with 256 nodes\n",
        "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "    \n",
        "    # Hidden layer with 64 nodes\n",
        "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    # Output Layer with # of actions: 2 nodes (left, right)\n",
        "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "\n",
        "    # X = Dense(16, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
        "    # X = Dense(16, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "    # X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "    model.compile(loss=\"mse\", optimizer=RMSprop(learning_rate=LEARNING_RATE, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "    model.summary()\n",
        "    print(\"End OurModel\")\n",
        "    return model"
      ],
      "id": "SMVyWAsn03cz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWOQpQCX1l5X"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, input_size, output_size, name = \"main\"):\n",
        "        self.state_size = input_size\n",
        "        self.action_size = output_size\n",
        "        self.net_name = name\n",
        "        \n",
        "        self.model = OurModel(input_shape=(self.state_size,), action_space=self.action_size)\n",
        "    \n",
        "    def predict(self, state):\n",
        "        return self.model.predict(np.reshape(state, [-1,4]))\n",
        "    \n",
        "    def fit(self, state, target):\n",
        "        self.model.fit(state, target,batch_size=BATCH_SIZE, verbose=0)"
      ],
      "id": "UWOQpQCX1l5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnSJiD1e018T"
      },
      "source": [
        "def replay(mainDQN, train_batch):\n",
        "\n",
        "    states = np.vstack([x[0] for x in train_batch])\n",
        "    actions = np.array([x[1] for x in train_batch])\n",
        "    rewards = np.array([x[2] for x in train_batch])\n",
        "    next_states = np.vstack([x[3] for x in train_batch])\n",
        "    dones = np.array([x[4] for x in train_batch])\n",
        "\n",
        "\n",
        "    target = mainDQN.predict(states)\n",
        "    target_next = mainDQN.predict(next_states)\n",
        "\n",
        "    for i in range(BATCH_SIZE):\n",
        "      if dones[i]:\n",
        "        target[i][actions[i]] = rewards[i]\n",
        "      else:\n",
        "        target[i][actions[i]] = rewards[i] + DISCOUNT_RATE * np.max(target_next[i])\n",
        "\n",
        "    mainDQN.fit(np.reshape(states, [-1,4]), target)"
      ],
      "id": "RnSJiD1e018T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D-fh1c42QX8"
      },
      "source": [
        "def run(mainDQN):\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "\n",
        "  while True:\n",
        "    env.render()\n",
        "    action = np.argmax(mainDQN.predict(state))\n",
        "    state, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    if done:\n",
        "      print(\"Total score: {}\".format(total_reward))\n",
        "      break"
      ],
      "id": "6D-fh1c42QX8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFRvi7U12UIr"
      },
      "source": [
        "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
        "\n",
        "    slope = (min_e - max_e) / (target_episode)\n",
        "    intercept = max_e\n",
        "\n",
        "    return max(min_e, slope * episode + intercept)"
      ],
      "id": "PFRvi7U12UIr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4a-rZn32cxy"
      },
      "source": [
        "replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "last_100_game_reward = deque(maxlen=100)\n",
        "\n",
        "mainDQN = DQN(INPUT_SIZE, OUTPUT_SIZE)\n",
        "\n",
        "for episode in range(MAX_EPISODE):\n",
        "    e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
        "    done = False\n",
        "\n",
        "    time_step = env.reset()\n",
        "    PIL.Image.fromarray(env.render())\n",
        "\n",
        "    state = time_step.observation\n",
        "\n",
        "    step_count = 0\n",
        "    while not done:\n",
        "        \n",
        "        # if np.random.rand() < e:\n",
        "        #     action = env.action_space.sample()\n",
        "        # else:\n",
        "        #     action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "        next_time_step = env.step(action)\n",
        "\n",
        "        # next_state, reward, done, info = env.step(action)\n",
        "        next_state = next_time_step.observation\n",
        "        reward = next_time_step.reward\n",
        "        done = next_time_step.is_last()\n",
        "\n",
        "        # if (step_count % 10 == 0):\n",
        "        #     print(\"state:\", state)\n",
        "        #     print(\"next_state:\", next_state)\n",
        "        #     print(\"reward:\", reward)\n",
        "        #     print(\"done:\", done)\n",
        "        \n",
        "\n",
        "        if done:\n",
        "            reward = -1\n",
        "        \n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        state = next_state\n",
        "        step_count += 1\n",
        "\n",
        "    if len(replay_buffer) > TRAIN_START:\n",
        "        minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
        "        replay(mainDQN, minibatch)\n",
        "\n",
        "    if (episode % 10 == 0):\n",
        "        print(\"[Episode {:>5}] steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
        "\n",
        "\n",
        "    last_100_game_reward.append(step_count)\n",
        "    if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
        "        avg_reward = np.mean(last_100_game_reward)\n",
        "        if avg_reward > 199.0:\n",
        "            print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
        "            break"
      ],
      "id": "q4a-rZn32cxy",
      "execution_count": null,
      "outputs": []
    }
  ]
}