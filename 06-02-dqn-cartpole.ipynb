{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "06-02-dqn-cartpole.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantae74/Reinforcement-Learning/blob/main/06-02-dqn-cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgQSbgOoe-2"
      },
      "source": [
        "모두를 위한 머신러닝에서 가져왔습니다.\n",
        "# DQN(Deep Q-Network) Cartpole"
      ],
      "id": "WfgQSbgOoe-2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK3QTbWDofS7"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "id": "RK3QTbWDofS7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dxHmEPog-b"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "id": "30dxHmEPog-b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlVW0eXwMZGR"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "id": "mlVW0eXwMZGR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4dApPH2MsCS"
      },
      "source": [
        "tf.version.VERSION"
      ],
      "id": "O4dApPH2MsCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-eb5N4sMx4q"
      },
      "source": [
        "# Hyperparmeters"
      ],
      "id": "I-eb5N4sMx4q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZxvFt3iMviR"
      },
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "id": "yZxvFt3iMviR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaJBX8UiNMW5"
      },
      "source": [
        "# Load the CartPole environment from the OpenAI Gym suite\n",
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)"
      ],
      "id": "YaJBX8UiNMW5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sVsmNS7OfFd"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "id": "2sVsmNS7OfFd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJIvtlCXVyq"
      },
      "source": [
        "In the Cartpole environment:\n",
        "\n",
        "* observation is an array of 4 floats:\n",
        "  * the position and velocity of the cart\n",
        "  * the angular position and velocity of the pole\n",
        "* reward is a scalar float value\n",
        "* action is a scalar integer with only two possible values:\n",
        "  * 0 — \"move left\"\n",
        "  * 1 — \"move right\""
      ],
      "id": "lLJIvtlCXVyq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSo9s_NKO5G4"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ],
      "id": "VSo9s_NKO5G4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l11skyeW1fP"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ],
      "id": "8l11skyeW1fP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoldkh07XGzh"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "id": "yoldkh07XGzh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrW1vhJiXNEC"
      },
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ],
      "id": "JrW1vhJiXNEC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZuoq3q_YiIh"
      },
      "source": [
        "# Usually two environments are instantiated: one for training and one for evaluation.\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ],
      "id": "wZuoq3q_YiIh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWQgb8fNY9lH"
      },
      "source": [
        "# The TFPyEnvironment converts these to Tensors to make it compatible with Tensorflow agents and policies.\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "id": "SWQgb8fNY9lH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CywkgVNwdgb_"
      },
      "source": [
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "\n",
        "At the heart of a DQN Agent is a QNetwork, a neural network model that can learn to predict QValues (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "We will use tf_agents.networks. to create a QNetwork. The network will consist of a sequence of tf.keras.layers.Dense layers, where the final layer will have 1 output for each possible action."
      ],
      "id": "CywkgVNwdgb_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXL_SFGwc8Vb"
      },
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# it's output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ],
      "id": "WXL_SFGwc8Vb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEib8zMognIL"
      },
      "source": [
        "# Now use tf_agents.agents.dqn.dqn_agent to instantiate a DqnAgent. \n",
        "# In addition to the time_step_spec, action_spec and the QNetwork, \n",
        "# the agent constructor also requires an optimizer (in this case, AdamOptimizer), \n",
        "# a loss function, and an integer step counter.\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "id": "tEib8zMognIL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrgD66p8hCKY"
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "id": "UrgD66p8hCKY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "576cES7ekbMp"
      },
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "id": "576cES7ekbMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbG2jI4pkoPQ"
      },
      "source": [
        "random_policy"
      ],
      "id": "AbG2jI4pkoPQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-BHSs1ukp7Y"
      },
      "source": [
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))"
      ],
      "id": "v-BHSs1ukp7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kehX9sohlMzP"
      },
      "source": [
        "time_step = example_environment.reset()"
      ],
      "id": "kehX9sohlMzP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS2tuVVzlPnP"
      },
      "source": [
        "random_policy.action(time_step)"
      ],
      "id": "LS2tuVVzlPnP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEJ5bIlSlRkX"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "id": "rEJ5bIlSlRkX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dU_ztaBm5sO"
      },
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "id": "1dU_ztaBm5sO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uSi6ecnnZr"
      },
      "source": [
        ""
      ],
      "id": "-4uSi6ecnnZr",
      "execution_count": null,
      "outputs": []
    }
  ]
}