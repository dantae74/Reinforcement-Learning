{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantae74/Reinforcement-Learning/blob/main/06-05-DQN-CartPole-2013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgQSbgOoe-2"
      },
      "source": [
        "모두를 위한 머신러닝에서 가져왔습니다."
      ],
      "id": "WfgQSbgOoe-2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK3QTbWDofS7"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ],
      "id": "RK3QTbWDofS7",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dxHmEPog-b"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "# env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
        "\n",
        "# Constants defining our neural network\n",
        "INPUT_SIZE = env.observation_space.shape[0]\n",
        "OUTPUT_SIZE = env.action_space.n\n",
        "\n",
        "DISCOUNT_RATE = 0.99\n",
        "REPLAY_MEMORY = 50000\n",
        "BATCH_SIZE = 64\n",
        "MAX_EPISODE = 5000\n",
        "TRAIN_START = 1000\n",
        "\n",
        "\n",
        "# minimum epsilon for epsilon greedy\n",
        "MIN_E = 0.0\n",
        "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
        "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01"
      ],
      "id": "30dxHmEPog-b",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK2Ab-6Q5Y0r"
      },
      "source": [
        "def OurModel(input_shape, action_space):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # 'Dense' is the basic form of a neural network layer\n",
        "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
        "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
        "\n",
        "    # Hidden layer with 256 nodes\n",
        "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "    \n",
        "    # Hidden layer with 64 nodes\n",
        "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    # Output Layer with # of actions: 2 nodes (left, right)\n",
        "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "#     model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ],
      "id": "WK2Ab-6Q5Y0r",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7njEVSbH5d_v"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, input_size, output_size, name = \"main\"):\n",
        "        self.state_size = input_size\n",
        "        self.action_size = output_size\n",
        "        self.net_name = name\n",
        "        \n",
        "        self.model = OurModel(INPUT_SIZE, OUTPUT_SIZE)\n",
        "    \n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)"
      ],
      "id": "7njEVSbH5d_v",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVTbJ_cE5dvO"
      },
      "source": [
        "def replay(mainDQN, train_batch):\n",
        "\n",
        "    states = np.vstack(x[0] for x in train_batch)\n",
        "    actions = np.array(x[1] for x in train_batch)\n",
        "    rewards = np.varray(x[2] for x in train_batch)\n",
        "    next_states = np.vstack(x[3] for x in train_batch)\n",
        "    dones = np.array(x[4] for x in train_batch)\n",
        "\n",
        "    target = mainDQN.predict(states)\n",
        "    target_next = mainDQN.predict(next_states)\n",
        "\n",
        "    for i in range(BATCH_SIZE):\n",
        "      if done[i]:\n",
        "        target[i][actions[i]] = reward[i]\n",
        "      else:\n",
        "        target[i][actions[i]] = rewards[i] + DISCOUNT_RATE * np.max(target_next[i])\n",
        "\n",
        "    mainDQN.fit(state, target,batch_size=BATCH_SIZE, verbose=0)"
      ],
      "id": "kVTbJ_cE5dvO",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYv4HYbVCDIa"
      },
      "source": [
        "def run(mainDQN):\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "\n",
        "  while True:\n",
        "    env.render()\n",
        "    action = np.argmax(mainDQN.predict(state))\n",
        "    state, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    if done:\n",
        "      print(\"Total score: {}\".format(total_reward))\n",
        "      break"
      ],
      "id": "YYv4HYbVCDIa",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sy7nrpfYxF8"
      },
      "source": [
        "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
        "    \"\"\"Return an linearly annealed epsilon\n",
        "    Epsilon will decrease over time until it reaches `target_episode`\n",
        "         (epsilon)\n",
        "             |\n",
        "    max_e ---|\\\n",
        "             | \\\n",
        "             |  \\\n",
        "             |   \\\n",
        "    min_e ---|____\\_______________(episode)\n",
        "                  |\n",
        "                 target_episode\n",
        "     slope = (min_e - max_e) / (target_episode)\n",
        "     intercept = max_e\n",
        "     e = slope * episode + intercept\n",
        "    Args:\n",
        "        episode (int): Current episode\n",
        "        min_e (float): Minimum epsilon\n",
        "        max_e (float): Maximum epsilon\n",
        "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
        "    Returns:\n",
        "        float: epsilon between `min_e` and `max_e`\n",
        "    \"\"\"\n",
        "\n",
        "    slope = (min_e - max_e) / (target_episode)\n",
        "    intercept = max_e\n",
        "\n",
        "    return max(min_e, slope * episode + intercept)"
      ],
      "id": "7Sy7nrpfYxF8",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlTgk7-TXi6i"
      },
      "source": [
        "def main():\n",
        "  replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "  last_100_game_reward = deque(maxlen=100)\n",
        "\n",
        "  mainDQN = DQN(INPUT_SIZE, OUTPUT_SIZE)\n",
        "\n",
        "  for episode in range(MAX_EPISODES):\n",
        "    e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "\n",
        "    step_count = 0\n",
        "    while not done:\n",
        "      if np.random.rand() < e:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "\n",
        "      if done:\n",
        "        reward = -1\n",
        "      \n",
        "      replay_buffer.append(state, action, reward, next_state, done)\n",
        "\n",
        "      state = next_state\n",
        "      step_count += 1\n",
        "\n",
        "      if len(replay_buffer) > TRAIN_START:\n",
        "        minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
        "        replay(mainDQN, minibatch)\n",
        "\n",
        "    print(\"[Episode {:>5}] steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
        "\n",
        "\n",
        "    last_100_game_reward.append(step_count)\n",
        "    if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
        "        avg_reward = np.mean(last_100_game_reward)\n",
        "        if avg_reward > 199.0:\n",
        "            print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
        "            break"
      ],
      "id": "VlTgk7-TXi6i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqM7ZvIPxRe-"
      },
      "source": [
        "if __name__ == \"__main__\"\n",
        "  main()"
      ],
      "id": "yqM7ZvIPxRe-",
      "execution_count": null,
      "outputs": []
    }
  ]
}