{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantae74/Reinforcement-Learning/blob/main/06-05-DQN-CartPole-2013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgQSbgOoe-2"
      },
      "source": [
        "모두를 위한 머신러닝에서 가져왔습니다."
      ],
      "id": "WfgQSbgOoe-2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK3QTbWDofS7"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ],
      "id": "RK3QTbWDofS7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dxHmEPog-b"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
        "\n",
        "# Constants defining our neural network\n",
        "INPUT_SIZE = env.observation_space.shape[0]\n",
        "OUTPUT_SIZE = env.action_space.n\n",
        "\n",
        "DISCOUNT_RATE = 0.99\n",
        "REPLAY_MEMORY = 50000\n",
        "BATCH_SIZE = 64\n",
        "TARGET_UPDATE_FREQUENCY = 5\n",
        "MAX_EPISODES = 5000\n",
        "TRAIN_START = 1000"
      ],
      "id": "30dxHmEPog-b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK2Ab-6Q5Y0r"
      },
      "source": [
        "def OurModel(input_shape, action_space):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # 'Dense' is the basic form of a neural network layer\n",
        "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
        "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
        "\n",
        "    # Hidden layer with 256 nodes\n",
        "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "    \n",
        "    # Hidden layer with 64 nodes\n",
        "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    # Output Layer with # of actions: 2 nodes (left, right)\n",
        "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "#     model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ],
      "id": "WK2Ab-6Q5Y0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7njEVSbH5d_v"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, input_size, output_size, name = \"main\"):\n",
        "        self.state_size = input_size\n",
        "        self.action_size = output_size\n",
        "        self.net_name = name\n",
        "        \n",
        "        self.model = OurModel(INPUT_SIZE, OUTPUT_SIZE)\n",
        "    \n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)"
      ],
      "id": "7njEVSbH5d_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVTbJ_cE5dvO"
      },
      "source": [
        "def replay(mainDQN, targetDQN, train_batch):\n",
        "    for state, action, reward, next_state, done in train_batch:"
      ],
      "id": "kVTbJ_cE5dvO",
      "execution_count": null,
      "outputs": []
    }
  ]
}