{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94daf7ef",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dantae74/Reinforcement-Learning/blob/main/06-05-DQN-CartPole-2013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WfgQSbgOoe-2",
   "metadata": {
    "id": "WfgQSbgOoe-2"
   },
   "source": [
    "모두를 위한 머신러닝에서 가져왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "RK3QTbWDofS7",
   "metadata": {
    "id": "RK3QTbWDofS7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30dxHmEPog-b",
   "metadata": {
    "id": "30dxHmEPog-b"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
    "\n",
    "# Constants defining our neural network\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPISODE = 5000\n",
    "TRAIN_START = 1000\n",
    "\n",
    "\n",
    "# minimum epsilon for epsilon greedy\n",
    "MIN_E = 0.0\n",
    "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
    "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "WK2Ab-6Q5Y0r",
   "metadata": {
    "id": "WK2Ab-6Q5Y0r"
   },
   "outputs": [],
   "source": [
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    \n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "#     model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7njEVSbH5d_v",
   "metadata": {
    "id": "7njEVSbH5d_v"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_size, output_size, name = \"main\"):\n",
    "        self.state_size = input_size\n",
    "        self.action_size = output_size\n",
    "        self.net_name = name\n",
    "        \n",
    "        self.model = OurModel(INPUT_SIZE, OUTPUT_SIZE)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kVTbJ_cE5dvO",
   "metadata": {
    "id": "kVTbJ_cE5dvO"
   },
   "outputs": [],
   "source": [
    "def replay(mainDQN, train_batch):\n",
    "\n",
    "    states = np.vstack(x[0] for x in train_batch)\n",
    "    actions = np.array(x[1] for x in train_batch)\n",
    "    rewards = np.varray(x[2] for x in train_batch)\n",
    "    next_states = np.vstack(x[3] for x in train_batch)\n",
    "    dones = np.array(x[4] for x in train_batch)\n",
    "\n",
    "    target = mainDQN.predict(states)\n",
    "    target_next = mainDQN.predict(next_states)\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "      if done[i]:\n",
    "        target[i][actions[i]] = reward[i]\n",
    "      else:\n",
    "        target[i][actions[i]] = rewards[i] + DISCOUNT_RATE * np.max(target_next[i])\n",
    "\n",
    "    mainDQN.fit(state, target,batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "YYv4HYbVCDIa",
   "metadata": {
    "id": "YYv4HYbVCDIa"
   },
   "outputs": [],
   "source": [
    "def run(mainDQN):\n",
    "  state = env.reset()\n",
    "  total_reward = 0\n",
    "\n",
    "  while True:\n",
    "    env.render()\n",
    "    action = np.argmax(mainDQN.predict(state))\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "      print(\"Total score: {}\".format(total_reward))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7Sy7nrpfYxF8",
   "metadata": {
    "id": "7Sy7nrpfYxF8"
   },
   "outputs": [],
   "source": [
    "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
    "    \"\"\"Return an linearly annealed epsilon\n",
    "    Epsilon will decrease over time until it reaches `target_episode`\n",
    "         (epsilon)\n",
    "             |\n",
    "    max_e ---|\\\n",
    "             | \\\n",
    "             |  \\\n",
    "             |   \\\n",
    "    min_e ---|____\\_______________(episode)\n",
    "                  |\n",
    "                 target_episode\n",
    "     slope = (min_e - max_e) / (target_episode)\n",
    "     intercept = max_e\n",
    "     e = slope * episode + intercept\n",
    "    Args:\n",
    "        episode (int): Current episode\n",
    "        min_e (float): Minimum epsilon\n",
    "        max_e (float): Maximum epsilon\n",
    "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
    "    Returns:\n",
    "        float: epsilon between `min_e` and `max_e`\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_e - max_e) / (target_episode)\n",
    "    intercept = max_e\n",
    "\n",
    "    return max(min_e, slope * episode + intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "VlTgk7-TXi6i",
   "metadata": {
    "id": "VlTgk7-TXi6i"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "  last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "  mainDQN = DQN(INPUT_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "  for episode in range(MAX_EPISODES):\n",
    "    e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    step_count = 0\n",
    "    while not done:\n",
    "      if np.random.rand() < e:\n",
    "        action = env.action_space.sample()\n",
    "      else:\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "\n",
    "      if done:\n",
    "        reward = -1\n",
    "      \n",
    "      replay_buffer.append(state, action, reward, next_state, done)\n",
    "\n",
    "      state = next_state\n",
    "      step_count += 1\n",
    "\n",
    "      if len(replay_buffer) > TRAIN_START:\n",
    "        minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "        replay(mainDQN, minibatch)\n",
    "\n",
    "    print(\"[Episode {:>5}] steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
    "\n",
    "\n",
    "    last_100_game_reward.append(step_count)\n",
    "    if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "        avg_reward = np.mean(last_100_game_reward)\n",
    "        if avg_reward > 199.0:\n",
    "            print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "yqM7ZvIPxRe-",
   "metadata": {
    "id": "yqM7ZvIPxRe-"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-20a4c88e53ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e5b869eed618>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mlast_100_game_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmainDQN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fbe6ea8a3a86>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOurModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c433ca85bd6>\u001b[0m in \u001b[0;36mOurModel\u001b[0;34m(input_shape, action_space)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 'Dense' is the basic form of a neural network layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Input Layer of state size(4) and Hidden Layer with 512 nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'he_uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Hidden layer with 256 nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ml/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m                \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m                **kwargs):\n\u001b[0;32m-> 1157\u001b[0;31m     super(Dense, self).__init__(\n\u001b[0m\u001b[1;32m   1158\u001b[0m         activity_regularizer=activity_regularizer, **kwargs)\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ml/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4409daa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
